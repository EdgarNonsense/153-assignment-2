{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e3bda8a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import torchaudio\n",
    "import json\n",
    "import os\n",
    "from transformers import T5Tokenizer, T5EncoderModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9090819d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Epoch 1/4 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 29/5937 [00:03<12:49,  7.67it/s, loss=9.25e+3]"
     ]
    }
   ],
   "source": [
    "# ========== Imports ==========\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchaudio\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import json\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from transformers import T5Tokenizer, T5EncoderModel\n",
    "\n",
    "# ========== Dataset ==========\n",
    "class MusicBenchDataset(Dataset):\n",
    "    def __init__(self, json_path, audio_base_path, tokenizer, max_length=512, sample_rate=16000, n_mels=80, frames=400, hop_size=256):\n",
    "        self.data = []\n",
    "        with open(json_path, 'r') as f:\n",
    "            for line in f:\n",
    "                self.data.append(json.loads(line))  # JSON lines format\n",
    "\n",
    "        self.audio_base_path = audio_base_path\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.sample_rate = sample_rate\n",
    "        self.n_mels = n_mels\n",
    "        self.frames = frames\n",
    "        self.hop_size = hop_size\n",
    "        self.mel_transform = torchaudio.transforms.MelSpectrogram(\n",
    "            sample_rate=self.sample_rate, n_mels=self.n_mels, hop_length=self.hop_size\n",
    "        )\n",
    "\n",
    "    def fix_mel_length(self, mel):\n",
    "        n_mels, frames = mel.shape\n",
    "        if frames > self.frames:\n",
    "            mel = mel[:, :self.frames]\n",
    "        elif frames < self.frames:\n",
    "            pad = torch.zeros((n_mels, self.frames - frames))\n",
    "            mel = torch.cat([mel, pad], dim=1)\n",
    "        return mel\n",
    "\n",
    "    def fix_wav_length(self, wav):\n",
    "        target_length = self.frames * self.hop_size\n",
    "        if wav.shape[0] > target_length:\n",
    "            wav = wav[:target_length]\n",
    "        elif wav.shape[0] < target_length:\n",
    "            pad = torch.zeros(target_length - wav.shape[0])\n",
    "            wav = torch.cat([wav, pad])\n",
    "        return wav\n",
    "\n",
    "    def preprocess_audio(self, filepath):\n",
    "        waveform, sr = torchaudio.load(filepath)\n",
    "        if sr != self.sample_rate:\n",
    "            waveform = torchaudio.functional.resample(waveform, sr, self.sample_rate)\n",
    "\n",
    "        mel = self.mel_transform(waveform).squeeze(0)  # (n_mels, time)\n",
    "        mel = self.fix_mel_length(mel)\n",
    "        waveform = waveform.squeeze(0)  # (samples)\n",
    "        waveform = self.fix_wav_length(waveform)\n",
    "\n",
    "        return mel, waveform\n",
    "\n",
    "    def preprocess_text(self, item):\n",
    "        text = f\"{item['main_caption']} {item['alt_caption']} {item['prompt_bpm']} {item['prompt_key']} {item['prompt_bt']} {item['prompt_ch']}\"\n",
    "        return text\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "        audio_path = os.path.join(self.audio_base_path, item['location'])\n",
    "\n",
    "        mel, wav = self.preprocess_audio(audio_path)\n",
    "\n",
    "        text = self.preprocess_text(item)\n",
    "        text_inputs = self.tokenizer(text, padding='max_length', truncation=True, max_length=self.max_length, return_tensors='pt')\n",
    "\n",
    "        bpm = item['bpm'] or 120.0  # Default if bpm is None\n",
    "        bpm = float(bpm) / 300.0  # normalize BPM\n",
    "\n",
    "        return {\n",
    "            'input_ids': text_inputs['input_ids'].squeeze(0),\n",
    "            'attention_mask': text_inputs['attention_mask'].squeeze(0),\n",
    "            'bpm': torch.tensor(bpm, dtype=torch.float32),\n",
    "            'mel': mel,\n",
    "            'wav': wav\n",
    "        }\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "# ========== Text-to-Mel Model ==========\n",
    "class TextToMelModel(nn.Module):\n",
    "    def __init__(self, text_embed_dim, mel_bins=80, frames=400):\n",
    "        super().__init__()\n",
    "        self.frames = frames\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(text_embed_dim + 1, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, mel_bins * frames)\n",
    "        )\n",
    "        self.mel_bins = mel_bins\n",
    "\n",
    "    def forward(self, text_embeddings, bpm):\n",
    "        x = torch.cat([text_embeddings, bpm.unsqueeze(1)], dim=1)\n",
    "        output = self.fc(x)\n",
    "        output = output.view(-1, self.mel_bins, self.frames)\n",
    "        return output\n",
    "\n",
    "# ========== SimpleMel2Wav Vocoder ==========\n",
    "class SimpleMel2Wav(nn.Module):\n",
    "    def __init__(self, n_mels=80, upsample_scales=[8, 8, 4, 1]):\n",
    "        super().__init__()\n",
    "        self.initial = nn.Conv1d(n_mels, 512, kernel_size=7, padding=3)\n",
    "        layers = []\n",
    "        in_channels = 512\n",
    "        for scale in upsample_scales:\n",
    "            layers.append(nn.ConvTranspose1d(\n",
    "                in_channels, in_channels // 2, kernel_size=scale * 2, stride=scale, padding=scale // 2))\n",
    "            layers.append(nn.LeakyReLU(0.2))\n",
    "            in_channels = in_channels // 2\n",
    "        self.upsample = nn.Sequential(*layers)\n",
    "        self.final = nn.Conv1d(in_channels, 1, kernel_size=7, padding=3)\n",
    "        self.tanh = nn.Tanh()\n",
    "\n",
    "    def forward(self, mel):\n",
    "        x = self.initial(mel)\n",
    "        x = self.upsample(x)\n",
    "        x = self.final(x)\n",
    "        x = self.tanh(x)\n",
    "        x = x.squeeze(1)  # [batch, time]\n",
    "\n",
    "        target_length = mel.shape[-1] * 256  # frames Ã— hop_size\n",
    "        if x.shape[1] > target_length:\n",
    "            x = x[:, :target_length]\n",
    "        elif x.shape[1] < target_length:\n",
    "            pad = torch.zeros((x.shape[0], target_length - x.shape[1]), device=x.device)\n",
    "            x = torch.cat([x, pad], dim=1)\n",
    "        return x\n",
    "\n",
    "\n",
    "# ========== Helper: Plot Mel Spectrogram ==========\n",
    "def plot_mel(mel, title=\"Mel Spectrogram\"):\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.imshow(mel.cpu().detach().numpy(), aspect='auto', origin='lower')\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"Frames\")\n",
    "    plt.ylabel(\"Mel Bins\")\n",
    "    plt.colorbar()\n",
    "    plt.show()\n",
    "\n",
    "# ========== Training ==========\n",
    "def train(mel_model, vocoder_model, text_encoder, dataloader, optimizer, mel_loss_fn, wav_loss_fn, device):\n",
    "    mel_model.train()\n",
    "    vocoder_model.train()\n",
    "    text_encoder.eval()\n",
    "\n",
    "    total_loss = 0\n",
    "    pbar = tqdm(dataloader, desc=\"Training\", leave=False)\n",
    "    for batch in pbar:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        bpm = batch['bpm'].to(device)\n",
    "        mel_target = batch['mel'].to(device)\n",
    "        wav_target = batch['wav'].to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            text_outputs = text_encoder(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            text_embeds = text_outputs.last_hidden_state.mean(dim=1)\n",
    "\n",
    "        mel_pred = mel_model(text_embeds, bpm)\n",
    "        wav_pred = vocoder_model(mel_pred)\n",
    "\n",
    "        mel_loss = mel_loss_fn(mel_pred, mel_target)\n",
    "        wav_loss = wav_loss_fn(wav_pred, wav_target)\n",
    "\n",
    "        loss = mel_loss + wav_loss\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        pbar.set_postfix(loss=loss.item())\n",
    "\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    return avg_loss\n",
    "\n",
    "# ========== Evaluation ==========\n",
    "def evaluate(mel_model, vocoder_model, text_encoder, dataloader, mel_loss_fn, wav_loss_fn, device):\n",
    "    mel_model.eval()\n",
    "    vocoder_model.eval()\n",
    "    text_encoder.eval()\n",
    "\n",
    "    total_loss = 0\n",
    "    pbar = tqdm(dataloader, desc=\"Evaluating\", leave=False)\n",
    "    with torch.no_grad():\n",
    "        for batch in pbar:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            bpm = batch['bpm'].to(device)\n",
    "            mel_target = batch['mel'].to(device)\n",
    "            wav_target = batch['wav'].to(device)\n",
    "\n",
    "            text_outputs = text_encoder(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            text_embeds = text_outputs.last_hidden_state.mean(dim=1)\n",
    "\n",
    "            mel_pred = mel_model(text_embeds, bpm)\n",
    "            wav_pred = vocoder_model(mel_pred)\n",
    "\n",
    "            mel_loss = mel_loss_fn(mel_pred, mel_target)\n",
    "            wav_loss = wav_loss_fn(wav_pred, wav_target)\n",
    "\n",
    "            loss = mel_loss + wav_loss\n",
    "            total_loss += loss.item()\n",
    "            pbar.set_postfix(loss=loss.item())\n",
    "\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    return avg_loss\n",
    "\n",
    "# ========== Inference ==========\n",
    "def generate(mel_model, vocoder_model, text_encoder, tokenizer, text_prompt, bpm_value, device):\n",
    "    mel_model.eval()\n",
    "    vocoder_model.eval()\n",
    "    text_encoder.eval()\n",
    "\n",
    "    inputs = tokenizer(text_prompt, return_tensors='pt', padding='max_length', truncation=True, max_length=512)\n",
    "    input_ids = inputs['input_ids'].to(device)\n",
    "    attention_mask = inputs['attention_mask'].to(device)\n",
    "    bpm_tensor = torch.tensor([bpm_value / 300.0], dtype=torch.float32).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        text_outputs = text_encoder(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        text_embeds = text_outputs.last_hidden_state.mean(dim=1)\n",
    "\n",
    "        mel_pred = mel_model(text_embeds, bpm_tensor)\n",
    "        plot_mel(mel_pred.squeeze(0), title=\"Generated Mel Spectrogram\")\n",
    "\n",
    "        wav = vocoder_model(mel_pred)\n",
    "        return wav\n",
    "\n",
    "# ========== Main ==========\n",
    "def main():\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    tokenizer = T5Tokenizer.from_pretrained('t5-small')\n",
    "    text_encoder = T5EncoderModel.from_pretrained('t5-small').to(device)\n",
    "\n",
    "    json_path = 'MusicBench_train.json'\n",
    "    audio_base_path = 'datashare'\n",
    "\n",
    "    full_dataset = MusicBenchDataset(\n",
    "        json_path=json_path,\n",
    "        audio_base_path=audio_base_path,\n",
    "        tokenizer=tokenizer\n",
    "    )\n",
    "\n",
    "    train_size = int(0.9 * len(full_dataset))\n",
    "    val_size = len(full_dataset) - train_size\n",
    "    train_dataset, val_dataset = random_split(full_dataset, [train_size, val_size])\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=8, shuffle=False)\n",
    "\n",
    "    mel_model = TextToMelModel(text_embed_dim=512, mel_bins=80, frames=400).to(device)\n",
    "    vocoder_model = SimpleMel2Wav(n_mels=80).to(device)\n",
    "\n",
    "    optimizer = torch.optim.Adam(list(mel_model.parameters()) + list(vocoder_model.parameters()), lr=1e-4)\n",
    "    mel_loss_fn = nn.MSELoss()\n",
    "    wav_loss_fn = nn.L1Loss()\n",
    "\n",
    "    epochs = 4\n",
    "    for epoch in range(epochs):\n",
    "        print(f\"\\n=== Epoch {epoch+1}/{epochs} ===\")\n",
    "        train_loss = train(mel_model, vocoder_model, text_encoder, train_loader, optimizer, mel_loss_fn, wav_loss_fn, device)\n",
    "        val_loss = evaluate(mel_model, vocoder_model, text_encoder, val_loader, mel_loss_fn, wav_loss_fn, device)\n",
    "        print(f\"Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}\")\n",
    "\n",
    "    # Inference Example\n",
    "    example_text = \"A calm piano melody with a soft beat and slow tempo in C major.\"\n",
    "    bpm_value = 90.0\n",
    "    wav = generate(mel_model, vocoder_model, text_encoder, tokenizer, example_text, bpm_value, device)\n",
    "\n",
    "    if wav is not None:\n",
    "        wav = wav.squeeze(0)  # Remove batch dimension if it's [1, time]\n",
    "        wav = wav.unsqueeze(0)  # Now [1, time] (mono)\n",
    "        torchaudio.save('generated_output.wav', wav.cpu(), 16000)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
